{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9850188,"sourceType":"datasetVersion","datasetId":6044027}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:29.077096Z","iopub.execute_input":"2024-11-09T15:01:29.077414Z","iopub.status.idle":"2024-11-09T15:01:33.505498Z","shell.execute_reply.started":"2024-11-09T15:01:29.077378Z","shell.execute_reply":"2024-11-09T15:01:33.504530Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%pip install pycocotools -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:33.507457Z","iopub.execute_input":"2024-11-09T15:01:33.507857Z","iopub.status.idle":"2024-11-09T15:01:46.199406Z","shell.execute_reply.started":"2024-11-09T15:01:33.507824Z","shell.execute_reply":"2024-11-09T15:01:46.197216Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CocoDetection\nfrom torch import nn, optim\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:46.201532Z","iopub.execute_input":"2024-11-09T15:01:46.202000Z","iopub.status.idle":"2024-11-09T15:01:47.437191Z","shell.execute_reply.started":"2024-11-09T15:01:46.201952Z","shell.execute_reply":"2024-11-09T15:01:47.436417Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Параметры\ndata_dir = '/kaggle/input/animalscropped'\ntrain_dir = f'{data_dir}/train'\nval_dir = f'{data_dir}/valid'\nbatch_size = 8\nnum_epochs = 60\nnum_classes = 1  # Установите количество классов в вашем датасете","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:47.438597Z","iopub.execute_input":"2024-11-09T15:01:47.439028Z","iopub.status.idle":"2024-11-09T15:01:47.443454Z","shell.execute_reply.started":"2024-11-09T15:01:47.438994Z","shell.execute_reply":"2024-11-09T15:01:47.442544Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:47.445765Z","iopub.execute_input":"2024-11-09T15:01:47.446110Z","iopub.status.idle":"2024-11-09T15:01:47.453669Z","shell.execute_reply.started":"2024-11-09T15:01:47.446077Z","shell.execute_reply":"2024-11-09T15:01:47.452774Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class CocoDataset(Dataset):\n    def __init__(self, img_dir, json_file, transform=None):\n        with open(json_file) as f:\n            self.coco_data = json.load(f)\n        self.img_dir = img_dir\n        self.transform = transform\n\n        # Создаем индекс аннотаций по id изображения для быстрого доступа\n        self.annotations_index = {}\n        for annotation in self.coco_data['annotations']:\n            image_id = annotation['image_id']\n            if image_id not in self.annotations_index:\n                self.annotations_index[image_id] = []\n            self.annotations_index[image_id].append(annotation)\n\n    def __len__(self):\n        return len(self.coco_data['images'])\n\n    def __getitem__(self, idx):\n        img_info = self.coco_data['images'][idx]\n        img_path = os.path.join(self.img_dir, img_info['file_name'])\n        image = Image.open(img_path).convert('RGB')\n\n        label = None\n        # Получаем аннотации для текущего изображения\n        if img_info['id'] in self.annotations_index:\n            annotations = self.annotations_index[img_info['id']]\n            for annotation in annotations:\n                label = 0 if annotation[\"category_id\"] == 1 else 1\n        if self.transform:\n            # image = self.transform(np.array(image))\n            # mask = self.transform(np.array(mask_image))\n            image = self.transform(image)\n        return image, torch.tensor(label).float().unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:47.454855Z","iopub.execute_input":"2024-11-09T15:01:47.455329Z","iopub.status.idle":"2024-11-09T15:01:47.465840Z","shell.execute_reply.started":"2024-11-09T15:01:47.455287Z","shell.execute_reply":"2024-11-09T15:01:47.464927Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Загрузка данных COCO\ntrain_dataset = CocoDataset(train_dir, f'{train_dir}/_annotations.coco.json', transform=transform)\nval_dataset = CocoDataset(val_dir, f'{val_dir}/_annotations.coco.json', transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:47.467080Z","iopub.execute_input":"2024-11-09T15:01:47.467660Z","iopub.status.idle":"2024-11-09T15:01:47.511399Z","shell.execute_reply.started":"2024-11-09T15:01:47.467618Z","shell.execute_reply":"2024-11-09T15:01:47.510693Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = torchvision.models.resnet101(pretrained=True)\n\n# Изменение выходного слоя для бинарной классификации\nmodel.fc = nn.Linear(model.fc.in_features, 1)  # Один нейрон для бинарной классификации\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\ncriterion = nn.BCEWithLogitsLoss()  \noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:47.512328Z","iopub.execute_input":"2024-11-09T15:01:47.512601Z","iopub.status.idle":"2024-11-09T15:01:49.682629Z","shell.execute_reply.started":"2024-11-09T15:01:47.512570Z","shell.execute_reply":"2024-11-09T15:01:49.681609Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n100%|██████████| 171M/171M [00:00<00:00, 204MB/s]  \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def train(model, train_loader, criterion, optimizer):\n    model.train()\n    running_loss = 0.0\n    for images, targets in train_loader:\n        images = images.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    epoch_loss = running_loss \n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n    return running_loss/len(train_loader)\n\n# Функция валидации\ndef validate(model, val_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, targets in val_loader:\n            targets = targets.to(device)\n\n            images = images.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n\n            running_loss += loss.item()\n            predicted = (torch.sigmoid(outputs) > 0.5).float()\n            total += len(targets)\n            correct += (predicted == targets).sum().item()\n\n    epoch_loss = running_loss / len(train_loader) \n    accuracy = correct / total\n    return epoch_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:49.683875Z","iopub.execute_input":"2024-11-09T15:01:49.684232Z","iopub.status.idle":"2024-11-09T15:01:49.694148Z","shell.execute_reply.started":"2024-11-09T15:01:49.684199Z","shell.execute_reply":"2024-11-09T15:01:49.693187Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Цикл обучения\nfor epoch in range(num_epochs):\n    train_loss = train(model, train_loader, criterion, optimizer)\n    val_loss, val_accuracy = validate(model, val_loader, criterion)\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n\nprint(\"Training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:01:49.695216Z","iopub.execute_input":"2024-11-09T15:01:49.695506Z","iopub.status.idle":"2024-11-09T15:26:14.175061Z","shell.execute_reply.started":"2024-11-09T15:01:49.695475Z","shell.execute_reply":"2024-11-09T15:26:14.174020Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/60], Loss: 0.6935\nEpoch 1/60, Train Loss: 0.6935, Val Loss: 0.1890, Val Accuracy: 0.6040\nEpoch [2/60], Loss: 0.6017\nEpoch 2/60, Train Loss: 0.6017, Val Loss: 0.1550, Val Accuracy: 0.7360\nEpoch [3/60], Loss: 0.6018\nEpoch 3/60, Train Loss: 0.6018, Val Loss: 0.1741, Val Accuracy: 0.7472\nEpoch [4/60], Loss: 0.5997\nEpoch 4/60, Train Loss: 0.5997, Val Loss: 0.1544, Val Accuracy: 0.7360\nEpoch [9/60], Loss: 0.5138\nEpoch 9/60, Train Loss: 0.5138, Val Loss: 0.1468, Val Accuracy: 0.7494\nEpoch [10/60], Loss: 0.5333\nEpoch 10/60, Train Loss: 0.5333, Val Loss: 0.1400, Val Accuracy: 0.7494\nEpoch [11/60], Loss: 0.4984\nEpoch 11/60, Train Loss: 0.4984, Val Loss: 0.1664, Val Accuracy: 0.7181\nEpoch [12/60], Loss: 0.4843\nEpoch 12/60, Train Loss: 0.4843, Val Loss: 0.1317, Val Accuracy: 0.7629\nEpoch [13/60], Loss: 0.4526\nEpoch 13/60, Train Loss: 0.4526, Val Loss: 0.2276, Val Accuracy: 0.6779\nEpoch [14/60], Loss: 0.4317\nEpoch 14/60, Train Loss: 0.4317, Val Loss: 0.1521, Val Accuracy: 0.7517\nEpoch [15/60], Loss: 0.4154\nEpoch 15/60, Train Loss: 0.4154, Val Loss: 0.1371, Val Accuracy: 0.7696\nEpoch [16/60], Loss: 0.3825\nEpoch 16/60, Train Loss: 0.3825, Val Loss: 0.1206, Val Accuracy: 0.8076\nEpoch [17/60], Loss: 0.3593\nEpoch 17/60, Train Loss: 0.3593, Val Loss: 0.1332, Val Accuracy: 0.7830\nEpoch [18/60], Loss: 0.3522\nEpoch 18/60, Train Loss: 0.3522, Val Loss: 0.1276, Val Accuracy: 0.8054\nEpoch [19/60], Loss: 0.3097\nEpoch 19/60, Train Loss: 0.3097, Val Loss: 0.1459, Val Accuracy: 0.7584\nEpoch [20/60], Loss: 0.3019\nEpoch 20/60, Train Loss: 0.3019, Val Loss: 0.1358, Val Accuracy: 0.7964\nEpoch [21/60], Loss: 0.2502\nEpoch 21/60, Train Loss: 0.2502, Val Loss: 0.1419, Val Accuracy: 0.8143\nEpoch [22/60], Loss: 0.2767\nEpoch 22/60, Train Loss: 0.2767, Val Loss: 0.1356, Val Accuracy: 0.7919\nEpoch [23/60], Loss: 0.2433\nEpoch 23/60, Train Loss: 0.2433, Val Loss: 0.1263, Val Accuracy: 0.8255\nEpoch [24/60], Loss: 0.1835\nEpoch 24/60, Train Loss: 0.1835, Val Loss: 0.1231, Val Accuracy: 0.8345\nEpoch [25/60], Loss: 0.1908\nEpoch 25/60, Train Loss: 0.1908, Val Loss: 0.1322, Val Accuracy: 0.8367\nEpoch [26/60], Loss: 0.1704\nEpoch 26/60, Train Loss: 0.1704, Val Loss: 0.1658, Val Accuracy: 0.8322\nEpoch [27/60], Loss: 0.1880\nEpoch 27/60, Train Loss: 0.1880, Val Loss: 0.1295, Val Accuracy: 0.8322\nEpoch [28/60], Loss: 0.1680\nEpoch 28/60, Train Loss: 0.1680, Val Loss: 0.1352, Val Accuracy: 0.8076\nEpoch [29/60], Loss: 0.1160\nEpoch 29/60, Train Loss: 0.1160, Val Loss: 0.1369, Val Accuracy: 0.8277\nEpoch [30/60], Loss: 0.1451\nEpoch 30/60, Train Loss: 0.1451, Val Loss: 0.1177, Val Accuracy: 0.8568\nEpoch [31/60], Loss: 0.1063\nEpoch 31/60, Train Loss: 0.1063, Val Loss: 0.1472, Val Accuracy: 0.8389\nEpoch [32/60], Loss: 0.1218\nEpoch 32/60, Train Loss: 0.1218, Val Loss: 0.1621, Val Accuracy: 0.8277\nEpoch [33/60], Loss: 0.1244\nEpoch 33/60, Train Loss: 0.1244, Val Loss: 0.1446, Val Accuracy: 0.8300\nEpoch [34/60], Loss: 0.1170\nEpoch 34/60, Train Loss: 0.1170, Val Loss: 0.1352, Val Accuracy: 0.8613\nEpoch [35/60], Loss: 0.1321\nEpoch 35/60, Train Loss: 0.1321, Val Loss: 0.1193, Val Accuracy: 0.8412\nEpoch [36/60], Loss: 0.0977\nEpoch 36/60, Train Loss: 0.0977, Val Loss: 0.1774, Val Accuracy: 0.8434\nEpoch [37/60], Loss: 0.1234\nEpoch 37/60, Train Loss: 0.1234, Val Loss: 0.1633, Val Accuracy: 0.8322\nEpoch [38/60], Loss: 0.0770\nEpoch 38/60, Train Loss: 0.0770, Val Loss: 0.1836, Val Accuracy: 0.8367\nEpoch [39/60], Loss: 0.1377\nEpoch 39/60, Train Loss: 0.1377, Val Loss: 0.1575, Val Accuracy: 0.8188\nEpoch [40/60], Loss: 0.0706\nEpoch 40/60, Train Loss: 0.0706, Val Loss: 0.1321, Val Accuracy: 0.8479\nEpoch [41/60], Loss: 0.0641\nEpoch 41/60, Train Loss: 0.0641, Val Loss: 0.1337, Val Accuracy: 0.8523\nEpoch [42/60], Loss: 0.0427\nEpoch 42/60, Train Loss: 0.0427, Val Loss: 0.1550, Val Accuracy: 0.8233\nEpoch [43/60], Loss: 0.0571\nEpoch 43/60, Train Loss: 0.0571, Val Loss: 0.1522, Val Accuracy: 0.8479\nEpoch [44/60], Loss: 0.1284\nEpoch 44/60, Train Loss: 0.1284, Val Loss: 0.1605, Val Accuracy: 0.8434\nEpoch [45/60], Loss: 0.0705\nEpoch 45/60, Train Loss: 0.0705, Val Loss: 0.1780, Val Accuracy: 0.8523\nEpoch [46/60], Loss: 0.0641\nEpoch 46/60, Train Loss: 0.0641, Val Loss: 0.1920, Val Accuracy: 0.8076\nEpoch [47/60], Loss: 0.1161\nEpoch 47/60, Train Loss: 0.1161, Val Loss: 0.1718, Val Accuracy: 0.8412\nEpoch [48/60], Loss: 0.0442\nEpoch 48/60, Train Loss: 0.0442, Val Loss: 0.1393, Val Accuracy: 0.8702\nEpoch [49/60], Loss: 0.0336\nEpoch 49/60, Train Loss: 0.0336, Val Loss: 0.1798, Val Accuracy: 0.8367\nEpoch [50/60], Loss: 0.0414\nEpoch 50/60, Train Loss: 0.0414, Val Loss: 0.1857, Val Accuracy: 0.8166\nEpoch [51/60], Loss: 0.0561\nEpoch 51/60, Train Loss: 0.0561, Val Loss: 0.1866, Val Accuracy: 0.8479\nEpoch [52/60], Loss: 0.0250\nEpoch 52/60, Train Loss: 0.0250, Val Loss: 0.1967, Val Accuracy: 0.8322\nEpoch [53/60], Loss: 0.0582\nEpoch 53/60, Train Loss: 0.0582, Val Loss: 0.1864, Val Accuracy: 0.8188\nEpoch [54/60], Loss: 0.0453\nEpoch 54/60, Train Loss: 0.0453, Val Loss: 0.2093, Val Accuracy: 0.8389\nEpoch [55/60], Loss: 0.0368\nEpoch 55/60, Train Loss: 0.0368, Val Loss: 0.1902, Val Accuracy: 0.8300\nEpoch [56/60], Loss: 0.0436\nEpoch 56/60, Train Loss: 0.0436, Val Loss: 0.2422, Val Accuracy: 0.7875\nEpoch [57/60], Loss: 0.0304\nEpoch 57/60, Train Loss: 0.0304, Val Loss: 0.1925, Val Accuracy: 0.7987\nEpoch [58/60], Loss: 0.0685\nEpoch 58/60, Train Loss: 0.0685, Val Loss: 0.2063, Val Accuracy: 0.8277\nEpoch [59/60], Loss: 0.0787\nEpoch 59/60, Train Loss: 0.0787, Val Loss: 0.1779, Val Accuracy: 0.8434\nEpoch [60/60], Loss: 0.0930\nEpoch 60/60, Train Loss: 0.0930, Val Loss: 0.1827, Val Accuracy: 0.8255\nTraining complete.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"torch.save(model, \"/kaggle/working/saved.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T15:26:14.176590Z","iopub.execute_input":"2024-11-09T15:26:14.176937Z","iopub.status.idle":"2024-11-09T15:26:14.484809Z","shell.execute_reply.started":"2024-11-09T15:26:14.176880Z","shell.execute_reply":"2024-11-09T15:26:14.483691Z"}},"outputs":[],"execution_count":11}]}